{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CG9D5K-JHOia",
        "outputId": "d518b91d-9ec9-4b81-ff0e-73b5df16955b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive',force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pohkssBIH1o6"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/speechbrain/speechbrain/\n",
        "%cd ./speechbrain\n",
        "!pip install -r requirements.txt\n",
        "!pip install -e ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yhzQ_e4sJJ7U",
        "outputId": "e68e4771-61d2-48b4-8fe9-1ceef6aa7f46"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: speechbrain in /usr/local/lib/python3.10/dist-packages (0.5.14)\n",
            "Requirement already satisfied: hyperpyyaml in /usr/local/lib/python3.10/dist-packages (from speechbrain) (1.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from speechbrain) (1.3.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from speechbrain) (1.23.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from speechbrain) (23.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from speechbrain) (1.10.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from speechbrain) (0.1.99)\n",
            "Requirement already satisfied: torch>=1.9 in /usr/local/lib/python3.10/dist-packages (from speechbrain) (2.0.1+cu118)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (from speechbrain) (2.0.2+cu118)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from speechbrain) (4.66.1)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from speechbrain) (0.16.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->speechbrain) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->speechbrain) (4.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->speechbrain) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->speechbrain) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->speechbrain) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->speechbrain) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.9->speechbrain) (3.27.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.9->speechbrain) (16.0.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->speechbrain) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->speechbrain) (2.31.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->speechbrain) (6.0.1)\n",
            "Requirement already satisfied: ruamel.yaml<=0.17.28,>=0.17.8 in /usr/local/lib/python3.10/dist-packages (from hyperpyyaml->speechbrain) (0.17.28)\n",
            "Requirement already satisfied: ruamel.yaml.clib>=0.2.7 in /usr/local/lib/python3.10/dist-packages (from ruamel.yaml<=0.17.28,>=0.17.8->hyperpyyaml->speechbrain) (0.2.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.9->speechbrain) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->speechbrain) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->speechbrain) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->speechbrain) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->speechbrain) (2023.7.22)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.9->speechbrain) (1.3.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.32.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.16.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.3.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.7.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n"
          ]
        }
      ],
      "source": [
        "!pip install speechbrain\n",
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9BZ3u3m8JMo-",
        "outputId": "37a4f718-0b7a-41ef-c9d9-36ee1dc3699b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyctcdecode in /usr/local/lib/python3.10/dist-packages (0.5.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from pyctcdecode) (1.23.5)\n",
            "Requirement already satisfied: pygtrie<3.0,>=2.1 in /usr/local/lib/python3.10/dist-packages (from pyctcdecode) (2.5.0)\n",
            "Requirement already satisfied: hypothesis<7,>=6.14 in /usr/local/lib/python3.10/dist-packages (from pyctcdecode) (6.82.6)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.10/dist-packages (from hypothesis<7,>=6.14->pyctcdecode) (23.1.0)\n",
            "Requirement already satisfied: sortedcontainers<3.0.0,>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from hypothesis<7,>=6.14->pyctcdecode) (2.4.0)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from hypothesis<7,>=6.14->pyctcdecode) (1.1.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install pyctcdecode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FOqRbM_dOkWA",
        "outputId": "d9701d28-a34e-400d-9a3e-e662b02e5ee5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting https://github.com/kpu/kenlm/archive/master.zip\n",
            "  Using cached https://github.com/kpu/kenlm/archive/master.zip (553 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install https://github.com/kpu/kenlm/archive/master.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "VgTUhQ3HJROU"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import torch\n",
        "import logging\n",
        "import speechbrain as sb\n",
        "from speechbrain.utils.distributed import run_on_main\n",
        "from hyperpyyaml import load_hyperpyyaml\n",
        "from pathlib import Path\n",
        "import torchaudio.transforms as T\n",
        "import torchaudio\n",
        "import numpy as np\n",
        "import kenlm\n",
        "from pyctcdecode import build_ctcdecoder\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A5RBz7wjJyv2",
        "outputId": "b38d2f35-de66-40f0-f046-c8aa645af33b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/.shortcut-targets-by-id/1osdzH_xbf3qWTH3TWpiTi_dARUqT-KAm/tunisian_corpora/tunisian_without_wavlm\n"
          ]
        }
      ],
      "source": [
        "%cd /content/drive/MyDrive/tunisian_corpora/tunisian_without_wavlm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 256
        },
        "id": "MlN5pIO7Jsvp",
        "outputId": "89776ccb-3e61-46ea-aae4-d7cd5ec31cdb"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-61bea4d851a9>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhparams_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_opts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverrides\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_arguments\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"./hparams/train_tunisian_withwavlm.yaml\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# If distributed_launch=True then\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# create ddp_group with the right communication protocol\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0msb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mddp_init_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_opts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'speechbrain' has no attribute 'parse_arguments'"
          ]
        }
      ],
      "source": [
        "hparams_file, run_opts, overrides = sb.parse_arguments([\"./hparams/train_tunisian_withwavlm.yaml\"])\n",
        "\n",
        "# If distributed_launch=True then\n",
        "# create ddp_group with the right communication protocol\n",
        "sb.utils.distributed.ddp_init_group(run_opts)\n",
        "\n",
        "with open(hparams_file) as fin:\n",
        "    hparams = load_hyperpyyaml(fin, overrides)\n",
        "\n",
        "# Create experiment directory\n",
        "sb.create_experiment_directory(\n",
        "    experiment_directory=hparams[\"output_folder\"],\n",
        "    hyperparams_to_save=hparams_file,\n",
        "    overrides=overrides,\n",
        ")\n",
        "def read_labels_file(labels_file):\n",
        "    with open(labels_file, \"r\",encoding=\"utf-8\") as lf:\n",
        "        lines = lf.read().splitlines()\n",
        "        division = \"===\"\n",
        "        numbers = {}\n",
        "        for line in lines :\n",
        "            if division in line :\n",
        "                break\n",
        "            string, number = line.split(\"=>\")\n",
        "            number = int(number)\n",
        "            string = string[1:-2]\n",
        "            numbers[number] = string\n",
        "        return [numbers[x] for x in range(len(numbers))]\n",
        "labels = read_labels_file(os.path.join(hparams[\"save_folder\"], \"label_encoder.txt\"))\n",
        "labels = [\"\"] + labels[1:-1] + [\"1\"]\n",
        "\n",
        "# Dataset prep (parsing Librispeech)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CtXTyXcoJwXO"
      },
      "outputs": [],
      "source": [
        "def dataio_prepare(hparams):\n",
        "    \"\"\"This function prepares the datasets to be used in the brain class.\n",
        "    It also defines the data processing pipeline through user-defined functions.\"\"\"\n",
        "\n",
        "    # 1. Define datasets\n",
        "    data_folder = hparams[\"data_folder\"]\n",
        "\n",
        "    train_data = sb.dataio.dataset.DynamicItemDataset.from_csv(\n",
        "        csv_path=hparams[\"train_csv\"], replacements={\"data_root\": data_folder},\n",
        "    )\n",
        "\n",
        "    if hparams[\"sorting\"] == \"ascending\":\n",
        "        # we sort training data to speed up training and get better results.\n",
        "        train_data = train_data.filtered_sorted(\n",
        "            sort_key=\"duration\",\n",
        "            key_max_value={\"duration\": hparams[\"avoid_if_longer_than\"]},\n",
        "        )\n",
        "        # when sorting do not shuffle in dataloader ! otherwise is pointless\n",
        "        hparams[\"dataloader_options\"][\"shuffle\"] = False\n",
        "\n",
        "    elif hparams[\"sorting\"] == \"descending\":\n",
        "        train_data = train_data.filtered_sorted(\n",
        "            sort_key=\"duration\",\n",
        "            reverse=True,\n",
        "            key_max_value={\"duration\": hparams[\"avoid_if_longer_than\"]},\n",
        "        )\n",
        "        # when sorting do not shuffle in dataloader ! otherwise is pointless\n",
        "        hparams[\"dataloader_options\"][\"shuffle\"] = False\n",
        "\n",
        "    elif hparams[\"sorting\"] == \"random\":\n",
        "        pass\n",
        "\n",
        "    else:\n",
        "        raise NotImplementedError(\n",
        "            \"sorting must be random, ascending or descending\"\n",
        "        )\n",
        "\n",
        "    valid_data = sb.dataio.dataset.DynamicItemDataset.from_csv(\n",
        "        csv_path=hparams[\"valid_csv\"], replacements={\"data_root\": data_folder},\n",
        "    )\n",
        "    # We also sort the validation data so it is faster to validate\n",
        "    valid_data = valid_data.filtered_sorted(sort_key=\"duration\")\n",
        "    test_datasets = {}\n",
        "    for csv_file in hparams[\"test_csv\"]:\n",
        "        name = Path(csv_file).stem\n",
        "        test_datasets[name] = sb.dataio.dataset.DynamicItemDataset.from_csv(\n",
        "            csv_path=csv_file, replacements={\"data_root\": data_folder}\n",
        "        )\n",
        "        test_datasets[name] = test_datasets[name].filtered_sorted(\n",
        "            sort_key=\"duration\"\n",
        "        )\n",
        "\n",
        "    datasets = [train_data, valid_data] + [i for k, i in test_datasets.items()]\n",
        "\n",
        "\n",
        "    # 2. Define audio pipeline:\n",
        "    @sb.utils.data_pipeline.takes(\"wav\")\n",
        "    @sb.utils.data_pipeline.provides(\"sig\")\n",
        "    def audio_pipeline(wav):\n",
        "        info = torchaudio.info(wav)\n",
        "        sig = sb.dataio.dataio.read_audio(wav)\n",
        "        resampled = torchaudio.transforms.Resample(\n",
        "            info.sample_rate, hparams[\"sample_rate\"],\n",
        "        )(sig)\n",
        "        return resampled\n",
        "\n",
        "    sb.dataio.dataset.add_dynamic_item(datasets, audio_pipeline)\n",
        "    label_encoder = sb.dataio.encoder.CTCTextEncoder()\n",
        "\n",
        "    # 3. Define text pipeline:\n",
        "    @sb.utils.data_pipeline.takes(\"wrd\")\n",
        "    @sb.utils.data_pipeline.provides(\n",
        "        \"wrd\", \"char_list\", \"tokens_list\", \"tokens\"\n",
        "    )\n",
        "    def text_pipeline(wrd):\n",
        "        yield wrd\n",
        "        char_list = list(wrd)\n",
        "        yield char_list\n",
        "        tokens_list = label_encoder.encode_sequence(char_list)\n",
        "        yield tokens_list\n",
        "        tokens = torch.LongTensor(tokens_list)\n",
        "        yield tokens\n",
        "\n",
        "    sb.dataio.dataset.add_dynamic_item(datasets, text_pipeline)\n",
        "    lab_enc_file = os.path.join(hparams[\"save_folder\"], \"label_encoder.txt\")\n",
        "    special_labels = {\n",
        "        \"blank_label\": hparams[\"blank_index\"],\n",
        "        \"unk_label\": hparams[\"unk_index\"]\n",
        "    }\n",
        "    label_encoder.load_or_create(\n",
        "        path=lab_enc_file,\n",
        "        from_didatasets=[train_data],\n",
        "        output_key=\"char_list\",\n",
        "        special_labels=special_labels,\n",
        "        sequence_input=True,\n",
        "    )\n",
        "\n",
        "    # 4. Set output:\n",
        "    sb.dataio.dataset.set_output_keys(\n",
        "        datasets, [\"id\", \"sig\", \"wrd\", \"char_list\", \"tokens\"],\n",
        "    )\n",
        "    return train_data, valid_data,test_datasets, label_encoder\n",
        "\n",
        "class ASR(sb.core.Brain):\n",
        "    def compute_forward(self, batch, stage):\n",
        "        \"\"\"Forward computations from the waveform batches to the output probabilities.\"\"\"\n",
        "\n",
        "        batch = batch.to(self.device)\n",
        "        wavs, wav_lens = batch.sig\n",
        "        wavs, wav_lens = wavs.to(self.device), wav_lens.to(self.device)\n",
        "\n",
        "        if stage == sb.Stage.TRAIN:\n",
        "            if hasattr(self.hparams, \"augmentation\"):\n",
        "                wavs = self.hparams.augmentation(wavs, wav_lens)\n",
        "\n",
        "        # Forward pass\n",
        "        feats = self.modules.wav2vec2(wavs, wav_lens)\n",
        "        x = self.modules.enc(feats)\n",
        "        logits = self.modules.ctc_lin(x)\n",
        "        p_ctc = self.hparams.log_softmax(logits)\n",
        "\n",
        "        return p_ctc, wav_lens\n",
        "\n",
        "    def custom_encode(self,wavs,wav_lens) :\n",
        "        wavs = wavs.to(self.device)\n",
        "        if(wav_lens is not None): wav_lens.to(self.device)\n",
        "\n",
        "        feats = self.modules.wav2vec2(wavs, wav_lens)\n",
        "        x = self.modules.enc(feats)\n",
        "        logits = self.modules.ctc_lin(x)\n",
        "        p_ctc = self.hparams.log_softmax(logits)\n",
        "\n",
        "        return feats,p_ctc\n",
        "\n",
        "\n",
        "\n",
        "    def compute_objectives(self, predictions, batch, stage):\n",
        "        \"\"\"Computes the loss (CTC) given predictions and targets.\"\"\"\n",
        "\n",
        "        p_ctc, wav_lens = predictions\n",
        "\n",
        "        ids = batch.id\n",
        "        tokens, tokens_lens = batch.tokens\n",
        "\n",
        "        loss = self.hparams.ctc_cost(p_ctc, tokens, wav_lens, tokens_lens)\n",
        "\n",
        "        if stage != sb.Stage.TRAIN:\n",
        "            predicted_tokens = sb.decoders.ctc_greedy_decode(\n",
        "                p_ctc, wav_lens, blank_id=self.hparams.blank_index\n",
        "            )\n",
        "            # Decode token terms to words\n",
        "            if self.hparams.use_language_modelling:\n",
        "                predicted_words = []\n",
        "                for logs in p_ctc:\n",
        "                    text = decoder.decode(logs.detach().cpu().numpy())\n",
        "                    predicted_words.append(text.split(\" \"))\n",
        "            else:\n",
        "                predicted_words = [\n",
        "                    \"\".join(self.tokenizer.decode_ndim(utt_seq)).split(\" \")\n",
        "                    for utt_seq in predicted_tokens\n",
        "                ]\n",
        "            # Convert indices to words\n",
        "            target_words = [wrd.split(\" \") for wrd in batch.wrd]\n",
        "\n",
        "            self.wer_metric.append(ids, predicted_words, target_words)\n",
        "            self.cer_metric.append(ids, predicted_words, target_words)\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def fit_batch(self, batch):\n",
        "        \"\"\"Train the parameters given a single batch in input\"\"\"\n",
        "        should_step = self.step % self.grad_accumulation_factor == 0\n",
        "        # Managing automatic mixed precision\n",
        "        # TOFIX: CTC fine-tuning currently is unstable\n",
        "        # This is certainly due to CTC being done in fp16 instead of fp32\n",
        "        if self.auto_mix_prec:\n",
        "            with torch.cuda.amp.autocast():\n",
        "                with self.no_sync():\n",
        "                    outputs = self.compute_forward(batch, sb.Stage.TRAIN)\n",
        "                loss = self.compute_objectives(outputs, batch, sb.Stage.TRAIN)\n",
        "            with self.no_sync(not should_step):\n",
        "                self.scaler.scale(\n",
        "                    loss / self.grad_accumulation_factor\n",
        "                ).backward()\n",
        "            if should_step:\n",
        "\n",
        "                if not self.hparams.wav2vec2.freeze:\n",
        "                    self.scaler.unscale_(self.wav2vec_optimizer)\n",
        "                self.scaler.unscale_(self.model_optimizer)\n",
        "                if self.check_gradients(loss):\n",
        "                    if not self.hparams.wav2vec2.freeze:\n",
        "                        if self.optimizer_step >= self.hparams.warmup_steps:\n",
        "                            self.scaler.step(self.wav2vec_optimizer)\n",
        "                    self.scaler.step(self.model_optimizer)\n",
        "                self.scaler.update()\n",
        "                self.zero_grad()\n",
        "                self.optimizer_step += 1\n",
        "        else:\n",
        "            # This is mandatory because HF models have a weird behavior with DDP\n",
        "            # on the forward pass\n",
        "            with self.no_sync():\n",
        "                outputs = self.compute_forward(batch, sb.Stage.TRAIN)\n",
        "\n",
        "            loss = self.compute_objectives(outputs, batch, sb.Stage.TRAIN)\n",
        "\n",
        "            with self.no_sync(not should_step):\n",
        "                (loss / self.grad_accumulation_factor).backward()\n",
        "            if should_step:\n",
        "                if self.check_gradients(loss):\n",
        "                    if not self.hparams.wav2vec2.freeze:\n",
        "                        if self.optimizer_step >= self.hparams.warmup_steps:\n",
        "                            self.wav2vec_optimizer.step()\n",
        "                    self.model_optimizer.step()\n",
        "                self.zero_grad()\n",
        "                self.optimizer_step += 1\n",
        "\n",
        "        self.on_fit_batch_end(batch, outputs, loss, should_step)\n",
        "        return loss.detach().cpu()\n",
        "\n",
        "    def evaluate_batch(self, batch, stage):\n",
        "        \"\"\"Computations needed for validation/test batches\"\"\"\n",
        "        predictions = self.compute_forward(batch, stage=stage)\n",
        "        with torch.no_grad():\n",
        "            loss = self.compute_objectives(predictions, batch, stage=stage)\n",
        "        return loss.detach()\n",
        "\n",
        "    def on_stage_start(self, stage, epoch):\n",
        "        \"\"\"Gets called at the beginning of each epoch\"\"\"\n",
        "        if stage != sb.Stage.TRAIN:\n",
        "            self.cer_metric = self.hparams.cer_computer()\n",
        "            self.wer_metric = self.hparams.error_rate_computer()\n",
        "\n",
        "    def on_stage_end(self, stage, stage_loss, epoch):\n",
        "        \"\"\"Gets called at the end of an epoch.\"\"\"\n",
        "        # Compute/store important stats\n",
        "        stage_stats = {\"loss\": stage_loss}\n",
        "        if stage == sb.Stage.TRAIN:\n",
        "            self.train_stats = stage_stats\n",
        "        else:\n",
        "            stage_stats[\"CER\"] = self.cer_metric.summarize(\"error_rate\")\n",
        "            stage_stats[\"WER\"] = self.wer_metric.summarize(\"error_rate\")\n",
        "\n",
        "        # Perform end-of-iteration things, like annealing, logging, etc.\n",
        "        if stage == sb.Stage.VALID:\n",
        "            old_lr_model, new_lr_model = self.hparams.lr_annealing_model(\n",
        "                stage_stats[\"loss\"]\n",
        "            )\n",
        "            old_lr_wav2vec, new_lr_wav2vec = self.hparams.lr_annealing_wav2vec(\n",
        "                stage_stats[\"loss\"]\n",
        "            )\n",
        "            sb.nnet.schedulers.update_learning_rate(\n",
        "                self.model_optimizer, new_lr_model\n",
        "            )\n",
        "            if not self.hparams.wav2vec2.freeze:\n",
        "                sb.nnet.schedulers.update_learning_rate(\n",
        "                    self.wav2vec_optimizer, new_lr_wav2vec\n",
        "                )\n",
        "            self.hparams.train_logger.log_stats(\n",
        "                stats_meta={\n",
        "                    \"epoch\": epoch,\n",
        "                    \"lr_model\": old_lr_model,\n",
        "                    \"lr_wav2vec\": old_lr_wav2vec,\n",
        "                },\n",
        "                train_stats=self.train_stats,\n",
        "                valid_stats=stage_stats,\n",
        "            )\n",
        "            self.checkpointer.save_and_keep_only(\n",
        "                meta={\"WER\": stage_stats[\"WER\"]}, min_keys=[\"WER\"],\n",
        "            )\n",
        "        elif stage == sb.Stage.TEST:\n",
        "            self.hparams.train_logger.log_stats(\n",
        "                stats_meta={\"Epoch loaded\": self.hparams.epoch_counter.current},\n",
        "                test_stats=stage_stats,\n",
        "            )\n",
        "            with open(self.hparams.wer_file, \"w\") as w:\n",
        "                self.wer_metric.write_stats(w)\n",
        "\n",
        "    def init_optimizers(self):\n",
        "        \"Initializes the wav2vec2 optimizer and model optimizer\"\n",
        "\n",
        "        # If the wav2vec encoder is unfrozen, we create the optimizer\n",
        "        if not self.hparams.wav2vec2.freeze:\n",
        "            self.wav2vec_optimizer = self.hparams.wav2vec_opt_class(\n",
        "                self.modules.wav2vec2.parameters()\n",
        "            )\n",
        "            if self.checkpointer is not None:\n",
        "                self.checkpointer.add_recoverable(\n",
        "                    \"wav2vec_opt\", self.wav2vec_optimizer\n",
        "                )\n",
        "\n",
        "        self.model_optimizer = self.hparams.model_opt_class(\n",
        "            self.hparams.model.parameters()\n",
        "        )\n",
        "\n",
        "        if self.checkpointer is not None:\n",
        "            self.checkpointer.add_recoverable(\"modelopt\", self.model_optimizer)\n",
        "\n",
        "    def zero_grad(self, set_to_none=False):\n",
        "        if not self.hparams.wav2vec2.freeze:\n",
        "            self.wav2vec_optimizer.zero_grad(set_to_none)\n",
        "        self.model_optimizer.zero_grad(set_to_none)\n",
        "\n",
        "\n",
        "\n",
        "label_encoder = sb.dataio.encoder.CTCTextEncoder()\n",
        "\n",
        "train_data, valid_data, test_datasets, label_encoder = dataio_prepare(\n",
        "        hparams\n",
        "    )\n",
        "\n",
        "\n",
        "# We dynamicaly add the tokenizer to our brain class.\n",
        "# NB: This tokenizer corresponds to the one used for the LM!!\n",
        "decoder = build_ctcdecoder(\n",
        "    labels,\n",
        "    kenlm_model_path=\"/content/drive/MyDrive/tunisian_corpora/tunisian_without_wavlm/lm_data/arpas/indomain.arpa\",  # either .arpa or .bin file\n",
        "    alpha=0.5,  # tuned on a val set\n",
        "    beta=1,  # tuned on a val set\n",
        ")\n",
        "\n",
        "asr_brain = ASR(\n",
        "    modules=hparams[\"modules\"],\n",
        "    hparams=hparams,\n",
        "    run_opts=run_opts,\n",
        "    checkpointer=hparams[\"checkpointer\"],\n",
        ")\n",
        "\n",
        "asr_brain.tokenizer = label_encoder\n",
        "\"\"\"\n",
        "# Testing\n",
        "real = True\n",
        "if real :\n",
        "    for k in test_datasets.keys():  # keys are test_clean, test_other etc\n",
        "        asr_brain.hparams.wer_file = os.path.join(\n",
        "            hparams[\"output_folder\"], \"wer_{}.txt\".format(k)\n",
        "        )\n",
        "        asr_brain.evaluate(\n",
        "            test_datasets[k], test_loader_kwargs=hparams[\"dataloader_options\"]\n",
        "        )\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y_c3SbkSbQsS"
      },
      "outputs": [],
      "source": [
        "from torch.nn.utils.rnn import pad_sequence\n",
        "def load_paths(wavs_path):\n",
        "    waveforms = []\n",
        "    for path in wavs_path :\n",
        "        waveform, _ = torchaudio.load(path)\n",
        "        waveforms.append(waveform.squeeze(0))\n",
        "    # normalize array length to the bigger arrays by pading with 0's\n",
        "    padded_arrays = pad_sequence(waveforms, batch_first=True)\n",
        "    return torch.tensor(padded_arrays)\n",
        "\n",
        "waveform = load_paths([\"/content/drive/MyDrive/tunisian_corpora/tunisian_without_wavlm/samples/Salah10.wav\",\"/content/drive/MyDrive/tunisian_corpora/tunisian_without_wavlm/samples/Salah10.wav\"])\n",
        "embeddings, posteriogram = asr_brain.custom_encode(waveform,None)\n",
        "print(embeddings.shape)\n",
        "print(posteriogram.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pLtJ4v-Wbx1H"
      },
      "outputs": [],
      "source": [
        "from speechbrain.pretrained import EncoderASR,EncoderDecoderASR\n",
        "import torchaudio\n",
        "import  speechbrain as sb\n",
        "import torch\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import torch\n",
        "import speechbrain as sb\n",
        "import numpy as np\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P6tTHHw3PKS6"
      },
      "outputs": [],
      "source": [
        "%ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8mRLJfpfLI03"
      },
      "outputs": [],
      "source": [
        "french_asr_model = EncoderASR.from_hparams(source=\"speechbrain/asr-wav2vec2-commonvoice-fr\", savedir=\"pretrained_models/asr-wav2vec2-commonvoice-fr\")\n",
        "english_asr_model = EncoderASR.from_hparams(source=\"speechbrain/asr-wav2vec2-commonvoice-en\", savedir=\"pretrained_models/asr-wav2vec2-commonvoice-en\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tqXX5Fk8LPtj"
      },
      "outputs": [],
      "source": [
        "#UTILS FUNCTIOJNS\n",
        "def get_size_dimensions(arr):\n",
        "    size_dimensions = []\n",
        "    while isinstance(arr, list):\n",
        "        size_dimensions.append(len(arr))\n",
        "        arr = arr[0]\n",
        "    return size_dimensions\n",
        "\n",
        "def scale_array(batch,n):\n",
        "    scaled_batch = []\n",
        "\n",
        "    for array in batch:\n",
        "        if(n < len(array)): raise ValueError(\"Cannot scale Array down\")\n",
        "\n",
        "        repeat = round(n/len(array))+1\n",
        "        scaled_length_array= []\n",
        "\n",
        "        for i in array:\n",
        "            for j in range(repeat) :\n",
        "                if(len(scaled_length_array) == n): break\n",
        "                scaled_length_array.append(i)\n",
        "\n",
        "        scaled_batch.append(scaled_length_array)\n",
        "\n",
        "    return torch.tensor(scaled_batch)\n",
        "\n",
        "\n",
        "def load_paths(wavs_path):\n",
        "    waveforms = []\n",
        "    for path in wavs_path :\n",
        "        waveform, _ = torchaudio.load(path)\n",
        "        waveforms.append(waveform.squeeze(0))\n",
        "    # normalize array length to the bigger arrays by pading with 0's\n",
        "    padded_arrays = pad_sequence(waveforms, batch_first=True)\n",
        "    return torch.tensor(padded_arrays)\n",
        "\n",
        "\n",
        "\n",
        "def word_to_vec(input_string):\n",
        "    mapping= {'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5, 'f': 6, 'g': 7, 'h': 8, 'i': 9, 'j': 10, 'k': 11, 'l': 12, 'm': 13, 'n': 14, 'o': 15, 'p': 16, 'q': 17, 'r': 18, 's': 19, 't': 20, 'u': 21, 'v': 22, 'w': 23, 'x': 24, 'y': 25, 'z': 26, 'ا': 27, 'ب': 28, 'ت': 29, 'ث': 30, 'ج': 31, 'ح': 32, 'خ': 33, 'د': 34, 'ذ': 35, 'ر': 36, 'ز': 37, 'س': 38, 'ش': 39, 'ص': 40, 'ض': 41, 'ط': 42, 'ظ': 43, 'ع': 44, 'غ': 45, 'ف': 46, 'ق': 47, 'ك': 48, 'ل': 49, 'م': 50, 'ن': 51, 'ه': 52, 'و': 53, 'ي': 54,' ':55}\n",
        "\n",
        "    numbers = [mapping[word] for word in input_string if word in mapping]\n",
        "    return numbers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uwiwCkpMLc2w"
      },
      "outputs": [],
      "source": [
        "device = 'cuda'\n",
        "verbose = 0\n",
        "#FLOW LEVEL FUNCTIONS\n",
        "def merge_strategy(embeddings1, embeddings2, embeddings3,post1, post2,post3):\n",
        "\n",
        "\n",
        "    post1 = post1.to(device)\n",
        "    post2 = post2.to(device)\n",
        "    post3 = post3.to(device)\n",
        "    embeddings1 = embeddings1.to(device)\n",
        "    embeddings2 = embeddings2.to(device)\n",
        "    embeddings3 = embeddings3.to(device)\n",
        "\n",
        "    posteriograms_merged = torch.cat((post1,post2,post3),dim=2)\n",
        "    embeddings_merged = torch.cat((embeddings1,embeddings2,embeddings3),dim=2)\n",
        "\n",
        "    if(verbose !=0):\n",
        "      print('MERGED POST ',posteriograms_merged.shape)\n",
        "      print('MERGED emb ',embeddings_merged.shape)\n",
        "\n",
        "    return torch.cat((posteriograms_merged,embeddings_merged),dim=2).to(device)\n",
        "\n",
        "def decode(model,wavs,wav_lens):\n",
        "\n",
        "    with torch.no_grad():\n",
        "        wav_lens = wav_lens.to(model.device)\n",
        "        encoder_out = model.encode_batch(wavs, wav_lens)\n",
        "        predictions = model.decoding_function(encoder_out, wav_lens)\n",
        "        return predictions\n",
        "\n",
        "def middle_layer(batch):\n",
        "    rel_length = torch.tensor([1.0 for x in batch])\n",
        "\n",
        "    tn_embeddings, tn_posteriogram = asr_brain.custom_encode(batch,None)\n",
        "\n",
        "    fr_embeddings = french_asr_model.mods.encoder.wav2vec2(batch)\n",
        "    fr_posteriogram =french_asr_model.encode_batch(batch,rel_length)\n",
        "\n",
        "    en_embeddings = english_asr_model.mods.encoder(batch)\n",
        "    en_posteriogram = english_asr_model.encode_batch(batch,rel_length)\n",
        "\n",
        "    if(verbose !=0):\n",
        "      print('[EMBEDDINGS] FR:',fr_embeddings.shape, \"EN:\",en_embeddings.shape, \"TN:\", tn_embeddings.shape)\n",
        "      print('[POSTERIOGRAM] FR:',fr_posteriogram.shape, \"EN:\",en_posteriogram.shape,\"TN:\",tn_posteriogram.shape)\n",
        "\n",
        "\n",
        "    bilangual_sample = merge_strategy(fr_embeddings,en_embeddings,tn_embeddings,fr_posteriogram,en_posteriogram,tn_posteriogram)\n",
        "    return bilangual_sample\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CagOITEjL1lq"
      },
      "outputs": [],
      "source": [
        "\n",
        "class Mixer(sb.core.Brain):\n",
        "\n",
        "    def compute_forward(self, batch, stage):\n",
        "        \"\"\"Forward computations from the waveform batches to the output probabilities.\"\"\"\n",
        "\n",
        "        wavs, wav_lens = batch.sig\n",
        "        wavs, wav_lens = wavs.to(self.device), wav_lens.to(self.device)\n",
        "\n",
        "        if stage == sb.Stage.TRAIN:\n",
        "            if hasattr(self.hparams, \"augmentation\"):\n",
        "                wavs = self.hparams.augmentation(wavs, wav_lens)\n",
        "\n",
        "        multi_langual_feats = middle_layer(wavs)\n",
        "        multi_langual_feats= multi_langual_feats.to(device)\n",
        "        feats, _ = self.modules.enc(multi_langual_feats)\n",
        "        logits = self.modules.ctc_lin(multi_langual_feats)\n",
        "        p_ctc = self.hparams.log_softmax(logits)\n",
        "\n",
        "        return p_ctc, wav_lens\n",
        "\n",
        "    def compute_objectives(self, predictions, batch, stage):\n",
        "        \"\"\"Computes the loss (CTC) given predictions and targets.\"\"\"\n",
        "\n",
        "        p_ctc, wav_lens = predictions\n",
        "\n",
        "        ids = batch.id\n",
        "        tokens, tokens_lens = batch.tokens\n",
        "\n",
        "        loss = self.hparams.ctc_cost(p_ctc, tokens, wav_lens, tokens_lens)\n",
        "\n",
        "        if stage != sb.Stage.TRAIN:\n",
        "            predicted_tokens = sb.decoders.ctc_greedy_decode(\n",
        "                p_ctc, wav_lens, blank_id=self.hparams.blank_index\n",
        "            )\n",
        "            # Decode token terms to words\n",
        "            if self.hparams.use_language_modelling:\n",
        "                predicted_words = []\n",
        "                for logs in p_ctc:\n",
        "                    text = decoder.decode(logs.detach().cpu().numpy())\n",
        "                    predicted_words.append(text.split(\" \"))\n",
        "            else:\n",
        "                predicted_words = [\n",
        "                    \"\".join(self.tokenizer.decode_ndim(utt_seq)).split(\" \")\n",
        "                    for utt_seq in predicted_tokens\n",
        "                ]\n",
        "            # Convert indices to words\n",
        "            target_words = [wrd.split(\" \") for wrd in batch.wrd]\n",
        "\n",
        "            self.wer_metric.append(ids, predicted_words, target_words)\n",
        "            self.cer_metric.append(ids, predicted_words, target_words)\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def fit_batch(self, batch):\n",
        "        \"\"\"Train the parameters given a single batch in input\"\"\"\n",
        "        should_step = self.step % self.grad_accumulation_factor == 0\n",
        "        # Managing automatic mixed precision\n",
        "        # TOFIX: CTC fine-tuning currently is unstable\n",
        "        # This is certainly due to CTC being done in fp16 instead of fp32\n",
        "        if self.auto_mix_prec:\n",
        "            with torch.cuda.amp.autocast():\n",
        "                with self.no_sync():\n",
        "                    outputs = self.compute_forward(batch, sb.Stage.TRAIN)\n",
        "                loss = self.compute_objectives(outputs, batch, sb.Stage.TRAIN)\n",
        "            with self.no_sync(not should_step):\n",
        "                self.scaler.scale(\n",
        "                    loss / self.grad_accumulation_factor\n",
        "                ).backward()\n",
        "            if should_step:\n",
        "\n",
        "\n",
        "                self.scaler.unscale_(self.model_optimizer)\n",
        "                if self.check_gradients(loss):\n",
        "                    self.scaler.step(self.model_optimizer)\n",
        "                self.scaler.update()\n",
        "                self.zero_grad()\n",
        "                self.optimizer_step += 1\n",
        "        else:\n",
        "            # This is mandatory because HF models have a weird behavior with DDP\n",
        "            # on the forward pass\n",
        "            with self.no_sync():\n",
        "                outputs = self.compute_forward(batch, sb.Stage.TRAIN)\n",
        "\n",
        "            loss = self.compute_objectives(outputs, batch, sb.Stage.TRAIN)\n",
        "\n",
        "            with self.no_sync(not should_step):\n",
        "                (loss / self.grad_accumulation_factor).backward()\n",
        "            if should_step:\n",
        "                if self.check_gradients(loss):\n",
        "                    self.model_optimizer.step()\n",
        "                self.zero_grad()\n",
        "                self.optimizer_step += 1\n",
        "\n",
        "        self.on_fit_batch_end(batch, outputs, loss, should_step)\n",
        "        return loss.detach().cpu()\n",
        "\n",
        "    def evaluate_batch(self, batch, stage):\n",
        "        \"\"\"Computations needed for validation/test batches\"\"\"\n",
        "        predictions = self.compute_forward(batch, stage=stage)\n",
        "        with torch.no_grad():\n",
        "            loss = self.compute_objectives(predictions, batch, stage=stage)\n",
        "        return loss.detach()\n",
        "\n",
        "    def on_stage_start(self, stage, epoch):\n",
        "        \"\"\"Gets called at the beginning of each epoch\"\"\"\n",
        "        if stage != sb.Stage.TRAIN:\n",
        "            self.cer_metric = self.hparams.cer_computer()\n",
        "            self.wer_metric = self.hparams.error_rate_computer()\n",
        "\n",
        "    def on_stage_end(self, stage, stage_loss, epoch):\n",
        "        \"\"\"Gets called at the end of an epoch.\"\"\"\n",
        "        # Compute/store important stats\n",
        "        stage_stats = {\"loss\": stage_loss}\n",
        "        if stage == sb.Stage.TRAIN:\n",
        "            self.train_stats = stage_stats\n",
        "        else:\n",
        "            stage_stats[\"CER\"] = self.cer_metric.summarize(\"error_rate\")\n",
        "            stage_stats[\"WER\"] = self.wer_metric.summarize(\"error_rate\")\n",
        "\n",
        "        # Perform end-of-iteration things, like annealing, logging, etc.\n",
        "        if stage == sb.Stage.VALID:\n",
        "            old_lr_model, new_lr_model = self.hparams.lr_annealing_model(\n",
        "                stage_stats[\"loss\"]\n",
        "            )\n",
        "            old_lr_wav2vec, new_lr_wav2vec = self.hparams.lr_annealing_wav2vec(\n",
        "                stage_stats[\"loss\"]\n",
        "            )\n",
        "            sb.nnet.schedulers.update_learning_rate(\n",
        "                self.model_optimizer, new_lr_model\n",
        "            )\n",
        "            if not self.hparams.wav2vec2.freeze:\n",
        "                sb.nnet.schedulers.update_learning_rate(\n",
        "                    self.wav2vec_optimizer, new_lr_wav2vec\n",
        "                )\n",
        "            self.hparams.train_logger.log_stats(\n",
        "                stats_meta={\n",
        "                    \"epoch\": epoch,\n",
        "                    \"lr_model\": old_lr_model,\n",
        "                    \"lr_wav2vec\": old_lr_wav2vec,\n",
        "                },\n",
        "                train_stats=self.train_stats,\n",
        "                valid_stats=stage_stats,\n",
        "            )\n",
        "            self.checkpointer.save_and_keep_only(\n",
        "                meta={\"WER\": stage_stats[\"WER\"]}, min_keys=[\"WER\"],\n",
        "            )\n",
        "        elif stage == sb.Stage.TEST:\n",
        "            self.hparams.train_logger.log_stats(\n",
        "                stats_meta={\"Epoch loaded\": self.hparams.epoch_counter.current},\n",
        "                test_stats=stage_stats,\n",
        "            )\n",
        "            with open(self.hparams.wer_file, \"w\") as w:\n",
        "                self.wer_metric.write_stats(w)\n",
        "\n",
        "    def init_optimizers(self):\n",
        "        \"Initializes the wav2vec2 optimizer and model optimizer\"\n",
        "\n",
        "        self.model_optimizer = self.hparams.model_opt_class(\n",
        "            self.hparams.model.parameters()\n",
        "        )\n",
        "\n",
        "        if self.checkpointer is not None:\n",
        "            self.checkpointer.add_recoverable(\"modelopt\", self.model_optimizer)\n",
        "\n",
        "    def zero_grad(self, set_to_none=False):\n",
        "\n",
        "        self.model_optimizer.zero_grad(set_to_none)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SRHgwoj7dLJi"
      },
      "outputs": [],
      "source": [
        "\n",
        "%pip install PyArabic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rBrElybKLwBN"
      },
      "outputs": [],
      "source": [
        "# ONLY RUN ONCE TO CLEAN UP THE dataset\n",
        "# SHOULD RUN AGAIN FOR DEV ? TEST ? TRAIN\n",
        "\"\"\"\n",
        "import csv\n",
        "import pyarabic.araby as araby\n",
        "\n",
        "# Open the input CSV file for reading and the output CSV file for writing\n",
        "input_file_path = '/content/drive/MyDrive/tunisian_corpora/code_switched/dev.csv'\n",
        "output_file_path = '/content/drive/MyDrive/tunisian_corpora/code_switched/dev_processed.csv'\n",
        "\n",
        "column_name = 'wrd'\n",
        "chars = set()\n",
        "# Define your processing function\n",
        "def process_data(data):\n",
        "    pattern = r\"<(?:en|fr)>|<\\/(?:en|fr)>|\\\\(?:en|fr)>|\\\\(?:en|fr)>|,|\\n|-|\\*|\\\\|'|>|\\b\\d+\\b|\\/|\\|\"\n",
        "    data= re.sub(pattern,\"\",data)\n",
        "    data = re.sub('\\ufeff|\"|<',\"\",data)\n",
        "    data = re.sub(\"ڨ\" , \"ق\",data)\n",
        "    data = araby.strip_diacritics(data)\n",
        "    return data.lower()\n",
        "\n",
        "# Open input CSV file for reading\n",
        "with open(input_file_path, 'r', newline='') as input_file:\n",
        "    csv_reader = csv.DictReader(input_file)\n",
        "    fieldnames = csv_reader.fieldnames\n",
        "\n",
        "    # Create a list to store processed rows\n",
        "    processed_rows = []\n",
        "\n",
        "    for row in csv_reader:\n",
        "        # Process the data in the input column\n",
        "        processed_data = process_data(row[column_name])\n",
        "\n",
        "        # Update the row with the processed data in the output column\n",
        "        row[column_name] = processed_data\n",
        "\n",
        "        for char in row[column_name]:\n",
        "          chars.add(char)\n",
        "\n",
        "        processed_rows.append(row)\n",
        "\n",
        "print(chars)\n",
        "print(len(chars))\n",
        "\n",
        "# Write the processed data to the output CSV file\n",
        "with open(output_file_path, 'w', newline='') as output_file:\n",
        "    csv_writer = csv.DictWriter(output_file, fieldnames=fieldnames)\n",
        "    csv_writer.writeheader()\n",
        "    csv_writer.writerows(processed_rows)\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GvS0l4Mi6O38"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OxRk9bxFMAHb"
      },
      "outputs": [],
      "source": [
        "hparams_file, run_opts, overrides = sb.parse_arguments([\"/content/drive/MyDrive/tunisian_corpora/tunisian_without_wavlm//hparams/mixer.yaml\"])\n",
        "\n",
        "# If distributed_launch=True then\n",
        "# create ddp_group with the right communication protocol\n",
        "sb.utils.distributed.ddp_init_group(run_opts)\n",
        "\n",
        "with open(hparams_file) as fin:\n",
        "    hparams = load_hyperpyyaml(fin, overrides)\n",
        "\n",
        "# Create experiment directory\n",
        "sb.create_experiment_directory(\n",
        "    experiment_directory=hparams[\"output_folder\"],\n",
        "    hyperparams_to_save=hparams_file,\n",
        "    overrides=overrides,\n",
        ")\n",
        "\n",
        "\n",
        "label_encoder = sb.dataio.encoder.CTCTextEncoder()\n",
        "\n",
        "train_data, valid_data, test_datasets, label_encoder = dataio_prepare(\n",
        "        hparams\n",
        "    )\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# We dynamicaly add the tokenizer to our brain class.\n",
        "# NB: This tokenizer corresponds to the one used for the LM!!\n",
        "\n",
        "decoder = build_ctcdecoder(\n",
        "    labels,\n",
        "    kenlm_model_path=\"/content/drive/MyDrive/tunisian_corpora/tunisian_without_wavlm/lm_data/arpas/indomain.arpa\",  # either .arpa or .bin file\n",
        "    alpha=0.5,  # tuned on a val set\n",
        "    beta=1,  # tuned on a val set\n",
        ")\n",
        "\n",
        "mixer = Mixer(\n",
        "    modules=hparams[\"modules\"],\n",
        "    hparams=hparams,\n",
        "    run_opts=run_opts,\n",
        "    checkpointer=hparams[\"checkpointer\"],\n",
        ")\n",
        "\n",
        "mixer.device = 'cpu'\n",
        "asr_brain.tokenizer = label_encoder\n",
        "\n",
        "\n",
        "\n",
        "mixer.fit(\n",
        "    mixer.hparams.epoch_counter,\n",
        "    train_data,\n",
        "    valid_data,\n",
        "    train_loader_kwargs=hparams[\"dataloader_options\"],\n",
        "    valid_loader_kwargs=hparams[\"test_dataloader_options\"],\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cWcfm-E2eMd5"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}